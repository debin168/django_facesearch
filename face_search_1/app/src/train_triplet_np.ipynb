{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AnacondaProjects\\facenet-master\\src\n",
      "--lfw_nrof_folds\n",
      "Namespace(alpha=0.2, batch_size=90, data_dir='~/datasets/casia/casia_maxpy_mtcnnalign_182_160', embedding_size=128, epoch_size=1000, gpu_memory_fraction=1.0, image_size=160, images_per_person=40, keep_probability=1.0, learning_rate=0.1, learning_rate_decay_epochs=100, learning_rate_decay_factor=1.0, learning_rate_schedule_file='data/learning_rate_schedule.txt', lfw_dir='~/datasets/lfw/lfw_realigned/', lfw_file_ext='png', lfw_nrof_folds=10, lfw_pairs='~/data/pairs.txt', logs_base_dir='~/logs/facenet', max_nrof_epochs=500, model_def='models.inception_resnet_v1', models_base_dir='~/models/facenet', moving_average_decay=0.9999, optimizer='ADAGRAD', people_per_batch=45, pretrained_model=None, random_crop=False, random_flip=False, seed=666, weight_decay=0.0)\n",
      "Model directory: C:\\Users\\Administrator/models/facenet\\20180113-145443\n",
      "Log directory: C:\\Users\\Administrator/logs/facenet\\20180113-145443\n",
      "LFW directory: ~/datasets/lfw/lfw_realigned/\n",
      "Skipped 6000 image pairs\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n",
      "Running forward pass on sampled images: 177.227\n",
      "Selecting suitable triplets for training\n",
      "(nrof_random_negs, nrof_triplets) = (937, 937): time=177.387 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_triplet.py:297: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][1/1000]\tTime 45.388\tLoss 0.369\n",
      "Epoch: [0][2/1000]\tTime 36.132\tLoss 0.403\n",
      "Epoch: [0][3/1000]\tTime 35.974\tLoss 0.388\n",
      "Epoch: [0][4/1000]\tTime 36.048\tLoss 0.306\n",
      "Epoch: [0][5/1000]\tTime 38.407\tLoss 0.321\n",
      "Epoch: [0][6/1000]\tTime 34.935\tLoss 0.549\n",
      "Epoch: [0][7/1000]\tTime 35.266\tLoss 0.357\n",
      "Epoch: [0][8/1000]\tTime 35.524\tLoss 0.380\n",
      "Epoch: [0][9/1000]\tTime 35.123\tLoss 0.340\n",
      "Epoch: [0][10/1000]\tTime 34.707\tLoss 0.302\n",
      "Epoch: [0][11/1000]\tTime 35.751\tLoss 0.414\n",
      "Epoch: [0][12/1000]\tTime 35.502\tLoss 0.319\n",
      "Epoch: [0][13/1000]\tTime 34.934\tLoss 0.310\n",
      "Epoch: [0][14/1000]\tTime 35.907\tLoss 0.374\n",
      "Epoch: [0][15/1000]\tTime 35.267\tLoss 0.298\n",
      "Epoch: [0][16/1000]\tTime 37.486\tLoss 0.338\n",
      "Epoch: [0][17/1000]\tTime 37.036\tLoss 0.324\n",
      "Epoch: [0][18/1000]\tTime 37.196\tLoss 0.290\n",
      "Epoch: [0][19/1000]\tTime 37.827\tLoss 0.265\n",
      "Epoch: [0][20/1000]\tTime 36.744\tLoss 0.289\n",
      "Epoch: [0][21/1000]\tTime 35.917\tLoss 0.300\n",
      "Epoch: [0][22/1000]\tTime 37.217\tLoss 0.283\n",
      "Epoch: [0][23/1000]\tTime 37.359\tLoss 0.300\n",
      "Epoch: [0][24/1000]\tTime 36.080\tLoss 0.162\n",
      "Epoch: [0][25/1000]\tTime 35.797\tLoss 0.155\n",
      "Epoch: [0][26/1000]\tTime 34.778\tLoss 0.129\n",
      "Epoch: [0][27/1000]\tTime 34.708\tLoss 0.340\n",
      "Epoch: [0][28/1000]\tTime 35.016\tLoss 0.197\n",
      "Epoch: [0][29/1000]\tTime 36.482\tLoss 0.227\n",
      "Epoch: [0][30/1000]\tTime 37.457\tLoss 0.146\n",
      "Epoch: [0][31/1000]\tTime 37.251\tLoss 0.164\n",
      "Epoch: [0][32/1000]\tTime 9.014\tLoss 0.060\n",
      "Running forward pass on sampled images: 177.077\n",
      "Selecting suitable triplets for training\n",
      "(nrof_random_negs, nrof_triplets) = (21, 21): time=177.127 seconds\n",
      "Epoch: [0][33/1000]\tTime 23.852\tLoss 0.368\n",
      "Running forward pass on sampled images: 178.571\n",
      "Selecting suitable triplets for training\n",
      "(nrof_random_negs, nrof_triplets) = (105, 105): time=178.631 seconds\n",
      "Epoch: [0][34/1000]\tTime 36.533\tLoss 0.463\n",
      "Epoch: [0][35/1000]\tTime 36.463\tLoss 0.186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-37afc88caf79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-37afc88caf79>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    187\u001b[0m                     \u001b[0mbatch_size_placeholder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_placeholder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase_train_placeholder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menqueue_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate_schedule_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m                     args.embedding_size, anchor, positive, negative, triplet_loss)\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;31m# Save variables and the metagraph if it doesn't exist already\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-37afc88caf79>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch, batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file, embedding_size, anchor, positive, negative, triplet_loss)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrof_examples\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mbatch_size_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase_train_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m             \u001b[0memb_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlab\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[0mloss_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Training a face recognizer with TensorFlow based on the FaceNet paper\n",
    "FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\n",
    "\"\"\"\n",
    "# MIT License\n",
    "# \n",
    "# Copyright (c) 2016 David Sandberg\n",
    "# \n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "# \n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib\n",
    "import itertools\n",
    "import argparse\n",
    "import facenet\n",
    "import lfw\n",
    "\n",
    "from tensorflow.python.ops import data_flow_ops\n",
    "\n",
    "from six.moves import xrange\n",
    "\n",
    "def main(args):\n",
    "  \n",
    "    network = importlib.import_module(args.model_def)\n",
    "\n",
    "    subdir = datetime.strftime(datetime.now(), '%Y%m%d-%H%M%S')\n",
    "    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n",
    "    if not os.path.isdir(log_dir):  # Create the log directory if it doesn't exist\n",
    "        os.makedirs(log_dir)\n",
    "    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n",
    "    if not os.path.isdir(model_dir):  # Create the model directory if it doesn't exist\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    # Write arguments to a text file\n",
    "    facenet.write_arguments_to_file(args, os.path.join(log_dir, 'arguments.txt'))\n",
    "        \n",
    "    # Store some git revision info in a text file in the log directory\n",
    " \n",
    "    #src_path,_ = os.path.split(os.path.realpath(__file__))\n",
    "    src_path,_= os.path.split(os.path.abspath(sys.argv[0]))\n",
    "    \n",
    "    facenet.store_revision_info(src_path, log_dir, ' '.join(sys.argv))\n",
    "\n",
    "    np.random.seed(seed=args.seed)\n",
    "    train_set = facenet.get_dataset(args.data_dir)\n",
    "    \n",
    "    print('Model directory: %s' % model_dir)\n",
    "    print('Log directory: %s' % log_dir)\n",
    "    if args.pretrained_model:\n",
    "        print('Pre-trained model: %s' % os.path.expanduser(args.pretrained_model))\n",
    "    \n",
    "    if args.lfw_dir:\n",
    "        print('LFW directory: %s' % args.lfw_dir)\n",
    "        # Read the file containing the pairs used for testing\n",
    "        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n",
    "        # Get the paths for the corresponding images\n",
    "        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs, args.lfw_file_ext)\n",
    "        \n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        tf.set_random_seed(args.seed)\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        # Placeholder for the learning rate\n",
    "        learning_rate_placeholder = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        \n",
    "        batch_size_placeholder = tf.placeholder(tf.int32, name='batch_size')\n",
    "        \n",
    "        phase_train_placeholder = tf.placeholder(tf.bool, name='phase_train')\n",
    "        \n",
    "        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name='image_paths')\n",
    "        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name='labels')\n",
    "        \n",
    "        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n",
    "                                    dtypes=[tf.string, tf.int64],\n",
    "                                    shapes=[(3,), (3,)],\n",
    "                                    shared_name=None, name=None)\n",
    "        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n",
    "        \n",
    "        nrof_preprocess_threads = 4\n",
    "        images_and_labels = []\n",
    "        for _ in range(nrof_preprocess_threads):\n",
    "            filenames, label = input_queue.dequeue()\n",
    "            images = []\n",
    "            for filename in tf.unstack(filenames):\n",
    "                file_contents = tf.read_file(filename)\n",
    "                image = tf.image.decode_image(file_contents, channels=3)\n",
    "                \n",
    "                if args.random_crop:\n",
    "                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n",
    "                else:\n",
    "                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n",
    "                if args.random_flip:\n",
    "                    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "                #pylint: disable=no-member\n",
    "                image.set_shape((args.image_size, args.image_size, 3))\n",
    "                images.append(tf.image.per_image_standardization(image))\n",
    "            images_and_labels.append([images, label])\n",
    "    \n",
    "        image_batch, labels_batch = tf.train.batch_join(\n",
    "            images_and_labels, batch_size=batch_size_placeholder, \n",
    "            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n",
    "            capacity=4 * nrof_preprocess_threads * args.batch_size,\n",
    "            allow_smaller_final_batch=True)\n",
    "        image_batch = tf.identity(image_batch, 'image_batch')\n",
    "        image_batch = tf.identity(image_batch, 'input')\n",
    "        labels_batch = tf.identity(labels_batch, 'label_batch')\n",
    "\n",
    "        # Build the inference graph\n",
    "        prelogits, _ = network.inference(image_batch, args.keep_probability, \n",
    "            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n",
    "            weight_decay=args.weight_decay)\n",
    "        \n",
    "        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name='embeddings')\n",
    "        # Split embeddings into anchor, positive and negative and calculate triplet loss\n",
    "        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n",
    "        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n",
    "        \n",
    "        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n",
    "            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "        # Calculate the total losses\n",
    "        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        total_loss = tf.add_n([triplet_loss] + regularization_losses, name='total_loss')\n",
    "\n",
    "        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n",
    "        train_op = facenet.train(total_loss, global_step, args.optimizer, \n",
    "            learning_rate, args.moving_average_decay, tf.global_variables())\n",
    "        \n",
    "        # Create a saver\n",
    "        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n",
    "\n",
    "        # Build the summary operation based on the TF collection of Summaries.\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        # Start running operations on the Graph.\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n",
    "\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n",
    "        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n",
    "\n",
    "        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "        coord = tf.train.Coordinator()\n",
    "        tf.train.start_queue_runners(coord=coord, sess=sess)\n",
    "\n",
    "        with sess.as_default():\n",
    "\n",
    "            if args.pretrained_model:\n",
    "                print('Restoring pretrained model: %s' % args.pretrained_model)\n",
    "                saver.restore(sess, os.path.expanduser(args.pretrained_model))\n",
    "\n",
    "            # Training and validation loop\n",
    "            epoch = 0\n",
    "            while epoch < args.max_nrof_epochs:\n",
    "                step = sess.run(global_step, feed_dict=None)\n",
    "                epoch = step // args.epoch_size\n",
    "                # Train for one epoch\n",
    "                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n",
    "                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n",
    "                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n",
    "                    args.embedding_size, anchor, positive, negative, triplet_loss)\n",
    "\n",
    "                # Save variables and the metagraph if it doesn't exist already\n",
    "                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n",
    "\n",
    "                # Evaluate on LFW\n",
    "                if args.lfw_dir:\n",
    "                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n",
    "                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n",
    "                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n",
    "\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n",
    "          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n",
    "          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n",
    "          embedding_size, anchor, positive, negative, triplet_loss):\n",
    "    batch_number = 0\n",
    "    \n",
    "    if args.learning_rate>0.0:\n",
    "        lr = args.learning_rate\n",
    "    else:\n",
    "        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n",
    "    while batch_number < args.epoch_size:\n",
    "        # Sample people randomly from the dataset\n",
    "        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n",
    "        \n",
    "        print('Running forward pass on sampled images: ', end='')\n",
    "        start_time = time.time()\n",
    "        nrof_examples = args.people_per_batch * args.images_per_person\n",
    "        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n",
    "        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n",
    "        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n",
    "        emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n",
    "        for i in range(nrof_batches):\n",
    "            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n",
    "            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n",
    "                learning_rate_placeholder: lr, phase_train_placeholder: True})\n",
    "            emb_array[lab,:] = emb\n",
    "        print('%.3f' % (time.time()-start_time))\n",
    "\n",
    "        # Select triplets based on the embeddings\n",
    "        print('Selecting suitable triplets for training')\n",
    "        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n",
    "            image_paths, args.people_per_batch, args.alpha)\n",
    "        selection_time = time.time() - start_time\n",
    "        print('(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds' % \n",
    "            (nrof_random_negs, nrof_triplets, selection_time))\n",
    "\n",
    "        # Perform training on the selected triplets\n",
    "        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n",
    "        triplet_paths = list(itertools.chain(*triplets))\n",
    "        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n",
    "        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n",
    "        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n",
    "        nrof_examples = len(triplet_paths)\n",
    "        train_time = 0\n",
    "        i = 0\n",
    "        emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "        loss_array = np.zeros((nrof_triplets,))\n",
    "        while i < nrof_batches:\n",
    "            start_time = time.time()\n",
    "            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n",
    "            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n",
    "            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n",
    "            emb_array[lab,:] = emb\n",
    "            loss_array[i] = err\n",
    "            duration = time.time() - start_time\n",
    "            print('Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f' %\n",
    "                  (epoch, batch_number+1, args.epoch_size, duration, err))\n",
    "            batch_number += 1\n",
    "            i += 1\n",
    "            train_time += duration\n",
    "            \n",
    "        # Add validation loss and accuracy to summary\n",
    "        summary = tf.Summary()\n",
    "        #pylint: disable=maybe-no-member\n",
    "        summary.value.add(tag='time/selection', simple_value=selection_time)\n",
    "        summary_writer.add_summary(summary, step)\n",
    "    return step\n",
    "  \n",
    "def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n",
    "    \"\"\" Select the triplets for training\n",
    "    \"\"\"\n",
    "    trip_idx = 0\n",
    "    emb_start_idx = 0\n",
    "    num_trips = 0\n",
    "    triplets = []\n",
    "    \n",
    "    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n",
    "    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n",
    "    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n",
    "    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n",
    "    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n",
    "    #  choosing the maximally violating example, as often done in structured output learning.\n",
    "\n",
    "    for i in xrange(people_per_batch):\n",
    "        nrof_images = int(nrof_images_per_class[i])\n",
    "        for j in xrange(1,nrof_images):\n",
    "            a_idx = emb_start_idx + j - 1\n",
    "            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n",
    "            for pair in xrange(j, nrof_images): # For every possible positive pair.\n",
    "                p_idx = emb_start_idx + pair\n",
    "                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n",
    "                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n",
    "                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n",
    "                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n",
    "                nrof_random_negs = all_neg.shape[0]\n",
    "                if nrof_random_negs>0:\n",
    "                    rnd_idx = np.random.randint(nrof_random_negs)\n",
    "                    n_idx = all_neg[rnd_idx]\n",
    "                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n",
    "                    #print('Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)' % \n",
    "                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n",
    "                    trip_idx += 1\n",
    "\n",
    "                num_trips += 1\n",
    "\n",
    "        emb_start_idx += nrof_images\n",
    "\n",
    "    np.random.shuffle(triplets)\n",
    "    return triplets, num_trips, len(triplets)\n",
    "\n",
    "def sample_people(dataset, people_per_batch, images_per_person):\n",
    "    nrof_images = people_per_batch * images_per_person\n",
    "  \n",
    "    # Sample classes from the dataset\n",
    "    nrof_classes = len(dataset)\n",
    "    class_indices = np.arange(nrof_classes)\n",
    "    np.random.shuffle(class_indices)\n",
    "    \n",
    "    i = 0\n",
    "    image_paths = []\n",
    "    num_per_class = []\n",
    "    sampled_class_indices = []\n",
    "    # Sample images from these classes until we have enough\n",
    "    while len(image_paths)<nrof_images:\n",
    "        class_index = class_indices[i]\n",
    "        nrof_images_in_class = len(dataset[class_index])\n",
    "        image_indices = np.arange(nrof_images_in_class)\n",
    "        np.random.shuffle(image_indices)\n",
    "        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n",
    "        idx = image_indices[0:nrof_images_from_class]\n",
    "        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n",
    "        sampled_class_indices += [class_index]*nrof_images_from_class\n",
    "        image_paths += image_paths_for_class\n",
    "        num_per_class.append(nrof_images_from_class)\n",
    "        i+=1\n",
    "  \n",
    "    return image_paths, num_per_class\n",
    "\n",
    "def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n",
    "        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n",
    "        nrof_folds, log_dir, step, summary_writer, embedding_size):\n",
    "    start_time = time.time()\n",
    "    # Run forward pass to calculate embeddings\n",
    "    print('Running forward pass on LFW images: ', end='')\n",
    "    \n",
    "    nrof_images = len(actual_issame)*2\n",
    "    assert(len(image_paths)==nrof_images)\n",
    "    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\n",
    "    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n",
    "    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n",
    "    emb_array = np.zeros((nrof_images, embedding_size))\n",
    "    nrof_batches = int(np.ceil(nrof_images / batch_size))\n",
    "    label_check_array = np.zeros((nrof_images,))\n",
    "    for i in xrange(nrof_batches):\n",
    "        batch_size = min(nrof_images-i*batch_size, batch_size)\n",
    "        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n",
    "            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\n",
    "        emb_array[lab,:] = emb\n",
    "        label_check_array[lab] = 1\n",
    "    print('%.3f' % (time.time()-start_time))\n",
    "    \n",
    "    assert(np.all(label_check_array==1))\n",
    "    \n",
    "    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n",
    "    \n",
    "    print('Accuracy: %1.3f+-%1.3f' % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print('Validation rate: %2.5f+-%2.5f @ FAR=%2.5f' % (val, val_std, far))\n",
    "    lfw_time = time.time() - start_time\n",
    "    # Add validation loss and accuracy to summary\n",
    "    summary = tf.Summary()\n",
    "    #pylint: disable=maybe-no-member\n",
    "    summary.value.add(tag='lfw/accuracy', simple_value=np.mean(accuracy))\n",
    "    summary.value.add(tag='lfw/val_rate', simple_value=val)\n",
    "    summary.value.add(tag='time/lfw', simple_value=lfw_time)\n",
    "    summary_writer.add_summary(summary, step)\n",
    "    with open(os.path.join(log_dir,'lfw_result.txt'),'at') as f:\n",
    "        f.write('%d\\t%.5f\\t%.5f\\n' % (step, np.mean(accuracy), val))\n",
    "\n",
    "def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n",
    "    # Save the model checkpoint\n",
    "    print('Saving variables')\n",
    "    start_time = time.time()\n",
    "    checkpoint_path = os.path.join(model_dir, 'model-%s.ckpt' % model_name)\n",
    "    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n",
    "    save_time_variables = time.time() - start_time\n",
    "    print('Variables saved in %.2f seconds' % save_time_variables)\n",
    "    metagraph_filename = os.path.join(model_dir, 'model-%s.meta' % model_name)\n",
    "    save_time_metagraph = 0  \n",
    "    if not os.path.exists(metagraph_filename):\n",
    "        print('Saving metagraph')\n",
    "        start_time = time.time()\n",
    "        saver.export_meta_graph(metagraph_filename)\n",
    "        save_time_metagraph = time.time() - start_time\n",
    "        print('Metagraph saved in %.2f seconds' % save_time_metagraph)\n",
    "    summary = tf.Summary()\n",
    "    #pylint: disable=maybe-no-member\n",
    "    summary.value.add(tag='time/save_variables', simple_value=save_time_variables)\n",
    "    summary.value.add(tag='time/save_metagraph', simple_value=save_time_metagraph)\n",
    "    summary_writer.add_summary(summary, step)\n",
    "  \n",
    "  \n",
    "def get_learning_rate_from_file(filename, epoch):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.split('#', 1)[0]\n",
    "            if line:\n",
    "                par = line.strip().split(':')\n",
    "                e = int(par[0])\n",
    "                lr = float(par[1])\n",
    "                if e <= epoch:\n",
    "                    learning_rate = lr\n",
    "                else:\n",
    "                    return learning_rate\n",
    "    \n",
    "\n",
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--logs_base_dir', type=str, \n",
    "        help='Directory where to write event logs.', default='~/logs/facenet')\n",
    "    parser.add_argument('--models_base_dir', type=str,\n",
    "        help='Directory where to write trained models and checkpoints.', default='~/models/facenet')\n",
    "    parser.add_argument('--gpu_memory_fraction', type=float,\n",
    "        help='Upper bound on the amount of GPU memory that will be used by the process.', default=1.0)\n",
    "    parser.add_argument('--pretrained_model', type=str,\n",
    "        help='Load a pretrained model before training starts.')\n",
    "    parser.add_argument('--data_dir', type=str,\n",
    "        help='Path to the data directory containing aligned face patches.',\n",
    "        default='~/datasets/casia/casia_maxpy_mtcnnalign_182_160')\n",
    "    parser.add_argument('--model_def', type=str,\n",
    "        help='Model definition. Points to a module containing the definition of the inference graph.', default='models.inception_resnet_v1')\n",
    "    parser.add_argument('--max_nrof_epochs', type=int,\n",
    "        help='Number of epochs to run.', default=500)\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "        help='Number of images to process in a batch.', default=90)\n",
    "    parser.add_argument('--image_size', type=int,\n",
    "        help='Image size (height, width) in pixels.', default=160)\n",
    "    parser.add_argument('--people_per_batch', type=int,\n",
    "        help='Number of people per batch.', default=45)\n",
    "    parser.add_argument('--images_per_person', type=int,\n",
    "        help='Number of images per person.', default=40)\n",
    "    parser.add_argument('--epoch_size', type=int,\n",
    "        help='Number of batches per epoch.', default=1000)\n",
    "    parser.add_argument('--alpha', type=float,\n",
    "        help='Positive to negative triplet distance margin.', default=0.2)\n",
    "    parser.add_argument('--embedding_size', type=int,\n",
    "        help='Dimensionality of the embedding.', default=128)\n",
    "    parser.add_argument('--random_crop', \n",
    "        help='Performs random cropping of training images. If false, the center image_size pixels from the training images are used. ' +\n",
    "         'If the size of the images in the data directory is equal to image_size no cropping is performed', action='store_true')\n",
    "    parser.add_argument('--random_flip', \n",
    "        help='Performs random horizontal flipping of training images.', action='store_true')\n",
    "    parser.add_argument('--keep_probability', type=float,\n",
    "        help='Keep probability of dropout for the fully connected layer(s).', default=1.0)\n",
    "    parser.add_argument('--weight_decay', type=float,\n",
    "        help='L2 weight regularization.', default=0.0)\n",
    "    parser.add_argument('--optimizer', type=str, choices=['ADAGRAD', 'ADADELTA', 'ADAM', 'RMSPROP', 'MOM'],\n",
    "        help='The optimization algorithm to use', default='ADAGRAD')\n",
    "    parser.add_argument('--learning_rate', type=float,\n",
    "        help='Initial learning rate. If set to a negative value a learning rate ' +\n",
    "        'schedule can be specified in the file \"learning_rate_schedule.txt\"', default=0.1)\n",
    "    parser.add_argument('--learning_rate_decay_epochs', type=int,\n",
    "        help='Number of epochs between learning rate decay.', default=100)\n",
    "    parser.add_argument('--learning_rate_decay_factor', type=float,\n",
    "        help='Learning rate decay factor.', default=1.0)\n",
    "    parser.add_argument('--moving_average_decay', type=float,\n",
    "        help='Exponential decay for tracking of training parameters.', default=0.9999)\n",
    "    parser.add_argument('--seed', type=int,\n",
    "        help='Random seed.', default=666)\n",
    "    parser.add_argument('--learning_rate_schedule_file', type=str,\n",
    "        help='File containing the learning rate schedule that is used when learning_rate is set to to -1.', default='data/learning_rate_schedule.txt')\n",
    "\n",
    "    # Parameters for validation on LFW\n",
    "    parser.add_argument('--lfw_pairs', type=str,\n",
    "        help='The file containing the pairs to use for validation.', default='~/data/pairs.txt')\n",
    "    parser.add_argument('--lfw_file_ext', type=str,\n",
    "        help='The file extension for the LFW dataset.', default='png', choices=['jpg', 'png'])\n",
    " \n",
    "    parser.add_argument('--lfw_dir', type=str,\n",
    "        help='Path to the data directory containing aligned face patches.', default='~/datasets/lfw/lfw_realigned/')\n",
    "    \n",
    "    parser.add_argument('--lfw_nrof_folds', type=int,\n",
    "        help='Number of folds to use for cross validation. Mainly used for testing.', default=10)\n",
    "    print('--lfw_nrof_folds')\n",
    "    return parser.parse_args(argv)\n",
    "  \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "pwd = os.getcwd()\n",
    "print(pwd) \n",
    "logs_base_dir=pwd+'/logs/facenet'\n",
    "models_base_dir=pwd+'/models/facenet'\n",
    "pretrained_model=pwd+'/20170511-185253.pb'\n",
    "data_dir=pwd+'/datasets/casia/casia_maxpy_mtcnnalign_182_160'\n",
    "sys.argv=['train_triplet.py']\n",
    "args=parse_arguments(sys.argv[1:])\n",
    "print(args)\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AnacondaProjects\\facenet-master\\src\n"
     ]
    }
   ],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
